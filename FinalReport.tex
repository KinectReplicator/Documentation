%1. Final Report: 10 pages + 2 pages on Current Trends in Robotics
%2. Interim Report: 5 pages + 1 page on Current Trends in Robotics
%3. Use latex
%4. Equations describing algorithms must be included.
%5. Analysis plots must be included. (For example, plot error as a function of some experimental parameter such as object distance).
%6. Include an image/photo of the robot
%7. Figures must be your own. No figures can be copied from the web or another periodical (even if it is cited).
%8. Figures must have sufucient captions and be referred to in the text (Using the latex ref command) The figure caption is very important. Discuss the main point of the figure within the caption. Captions are often several sentences.
%9. Label all axes and variables in plots.


%10. For final report: a submission of demo video is required.
%11. Poster is required for department demo day.
%12. Separate from the final report, each person must individually submit a paragraph describing his or her contribution to the project. The tasks of the project should be well distributed (for example, do not have one team member do the all the writing and the other do all the programming).


\documentclass[12pt,twocolumn]{article}
\usepackage[upright]{fourier} 
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tkz-euclide} 
\usepackage{amsmath}
\usetkzobj{all} 
\usepackage{float}
\restylefloat{table}
\begin{document}
\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\center
\textsc{\LARGE Rutgers University}\\[1.5cm] 
\textsc{\Large Capstone Design}\\[0.5cm] 
\textsc{\large Electrical and Computer Engineering}\\[0.5cm]
\HRule \\[0.4cm]
{ \huge \bfseries Computer Vision-Based 3D Reconstruction for Object Replication}\\[0.4cm] 
\HRule \\[1.5cm]
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Authors:}\\
Ryan \textsc{Cullinane}\\
Cady \textsc{Motyka}\\
Elie \textsc{Rosen}
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Kristen \textsc{Dana} 
\end{flushright}
\end{minipage}\\[4cm]
{\large \today}\\[3cm]
\vfill 
\end{titlepage}

\section{Introduction}
% A statement of the problem and its major components in your own words.
This is the Introduction to
\section{Methods}
% A description of how each component in the objective is achieved.

{ \bf Calibration}\\
{ \bf Stereo Reconstruction}\\ 

\begin{tikzpicture}
\tkzDefPoint(0,0){O_{L}}\tkzDefPoint(6,0){O_{R}}
\tkzDefPoint(3,4){P}\tkzDefPoint(-.5,.5){leftL}
\tkzDefPoint(1.5,.5){leftR}\tkzDefPoint(4.5,.5){rightL}
\tkzDefPoint(6.5,.5){rightR}\tkzDefPoint(3,0){B}
\tkzDefPoint(3,.5){A}

\tkzDefPoint(.38,.5){P_{L}}
\tkzDefPoint(5.6,.5){P_{R}}

\tkzDrawSegments(O_{L},P P,O_{R} O_{R},O_{L} leftL,leftR rightL,rightR B,P)
\tkzDrawPoint(A)
\tkzDrawPoint(B)
\tkzDrawPoint(P_{L})
\tkzDrawPoint(P_{R})
\tkzLabelPoints[below](O_{L},O_{R})
\tkzLabelPoints[above](P)
\tkzLabelPoints[above left](P_{L})

\tkzLabelPoints[above right](P_{R})
\tkzLabelPoints[above right](A)

\tkzLabelPoints[below right](B)
\end{tikzpicture}  

P is the location of the object in the world, $O_{L}$ and $O_{R}$ are the left and right camera centers, $P_{R}$ and $P_{L}$ are the appearance of the point P in the two image planes where $P_{L}= \begin{bmatrix}
x_{L} \\
y_{L} \end{bmatrix} $. The distance between $O_{L}$ and $O_{R}$ is T, or the distance between the left and right camera. The distance between A and B is the focal length of the cameras. If we define the distance between P and B as distance Z, the following equation  can be used to represent the ratio between T and Z, using the theorem of like triangles: \center
$\frac{T}{Z}=\frac{T+x_{L}-x_{R}}{Z-f} or \frac{T-{x_{R}-x_{L}}}{Z-f} $ \\ \flushleft
Cross multiplying these equations results in: \center
$\frac{Z(T-x_{R}-x_{L})}{Z-f} = \frac{T(Z-f)}{Z} $ \flushleft
These calculations show that depth, or Z, is inversely proportional to disparity. This means that  $P_{L}= \frac{f^{L}P}{Z_{L}}. $
\section{Experimental Results}
%Measurements, repeated trials (for validation), error/performance analysis (as a function of system parameters). Include plots, images or tables to describe measurement values.

\section{Discussion}
%Discuss difficulties, sources of error, future work and extensions.

\section{Cost Analysis}
%Discuss the cost entailed with a product design based on your work. Use examples from currently available equipment in your cost estimates.
\begin{table}[H]
    \begin{tabular}{|p{2.5cm}|l|p{4.5cm}|}
\hline
        Item                            & Cost      & Purpose                                                                                                             \\  \hline
        Printbot LC                     & \S 549.00 & 3D Printer for developing reconstructed objects                                                                     \\ \hline
        Microsoft Kinect                & \S 109.99 & The most powerful sensor for its price, contains color filtering and depth filtering in one package                 \\  \hline
        ikg 3mm ABS Spool               & \S 46.00  & Material that the 2D Printer uses for creating objects, spool is necessary for feeding into the printer             \\ \hline
        2x 1lb 3mm ABS                  & \S 36.00  & Extra printing material, no extra spool required                                                                    \\ \hline
        Arduino Leonardo                & \S 24.95  & Microcontroller to plan orientation of object during reconstruction process                                         \\ \hline
        EasyDriver Stepper Motor Driver & \S 14.95  & Allows the ability to control a stepper motion at lower voltages such as from an Arduino microcontroller            \\ \hline
        Stepper Motor with Cable        & \S 14.95  & Motor that can be controlled in "steps" this allows a more precise method for orienting objects being reconstructed \\ \hline 
        Total:                          & \S 795.84  & ~         \\ \hline                                                                                           
\end{tabular}
\end{table}




\section{Current Trends in Robotics and Computer Vision}
% Describe real world robotic systems of research programs that are related to your capstone project. Research the literature and provide formal citations from publications (as obtained from IEEE Xplore or ACM Digital Library on the Rutgers library site) and periodicals (e.g. NY Times, Wall Street Journal). Do not use websites as sources for this section.
One of the reasons that the Kinect has become so popular for computer vision projects is that it is a cheap, quick, and highly reliable for 3D measurements. Many researchers are beginning to look into the possibility of using this device to achieve everything from a 3D reconstruction of a scene to aiding in a SLAM algorithm. The fact that this device is so affordable, and so many new resources are available, makes the Kinect a viable device for conducting research in the field of robotics and computer vision.\\
%https://docs.google.com/file/d/0B6Kc0pBSSJ79UHF0U2h0d2E5NDQ/edit
\indent The KinectFusion Project is slightly different than other projects that were using the Kinect; instead of using both the RGB cameras and the sensor, this project tracks the 3D sensor pose and preforms a reconstruction in real time using exclusively the depth data. This paper points out that depth cameras aren't exactly new, but the Kinect is a low-cost, real-time, depth camera that is much more accessible. The accuracy of the Kinect is called into questions, the point cloud that the depth data creates does usually contain noise and sometimes has holes where no readings were obtained. This project also considered the Kinect's low X/Y resolution and depth accuracy and fixes the quality of the images using depth super resolution. KinectFusiont also looks into using multiple Kinects to preform a 3D body scan; this raises more issues because the quality of the overlapping sections of the images is compromised. \\
%https://docs.google.com/file/d/0B6Kc0pBSSJ79eHdOZ2dxZ3JseW8/edit
\indent	Another KinectFusion Project is the Real-time 3D Reconstruction and Interaction, this project is impressive because the entire process is done using a moving depth camera. With this software, the user can hold a Kinect camera up to a scene, and a 3D construction would be made. Not only would the user be able to see the 3D Reconstruction, but they would be able to interact with it; for instance, if they were to throw a handful of spheres onto the scene, they would land on the top of appropriate surfaces, and fall under appropriate objects following the rules of physics. To accomplish this, the depth camera is used to track the 3D pose and the sensor is used to reconstruct the scene. Different views of the scene are taken and fussed together into a singe representation, the pipe line segments the objects in the scene and uses them to create a global surface based reconstruction. This project shows the real-time capabilities of then Kinect and why that makes it an innovative tool for computer vision.\\
%http://download.springer.com/static/pdf/111/chp%253A10.1007%252F978-1-4471-4640-7_1.pdf auth66=1362330926_cd812bb15c7056eaaf2e0d67d1235b82&ext=.pdf
\indent	 A study shown in the Asia Simulation Conference in 2011 demonstrated that a calibrated Kinect can be combined with Structure from Motion to find the 3D data of a scene and reconstruct the surface by Multiview Stereo. This study proved that the Kinect was more accurate for this procedure than a SwissRanger SR-4000 3D-TOF camera and close to a medium resolution SLR Stereo rig. The Kinect works by using a near-infrared laser pattern projector and an IR camera as a stereo pair to triangulate points in 3D space, then the RGB camera is used to reconstruct the correct texture to the 3D points. This RGB camera, which outputs medium quality images, can also be used for recognition. One issue this study found was that the resulting IR and Depth images were shifted. To figure out what the shift was, the Kinect recorded pictures of a circle from different distances. The shift was found to be around 4 pixels in the \emph{u} direction and three pixels in the \emph{v} direction. Even after the camera has been totally calibrated, there are a few remaining residual errors in the close range 3D measurements. An easy fix for this error was to we form a \emph{z}-correction image of \emph{z} values constructed as the pixel-wise mean of all residual images and then subtract that correction image from the \emph{z} coordinates of the 3D image.  \cite{cite1} Though the SLR Stereo was the most accurate, the error e (or the Euclidean distance between the points returned by the sensors and points reconstructed in the process of calibration) of the SR-400 was much higher than the Kinect and the SLR. This study shows that the Kinect is possible cheaper and simpler alternative to previously used cameras and rigs in the computer vision field.\\
%http://download.springer.com/static/pdf/132/chp%253A10.1007%252F978-4-431-54216-2_24.pdf?auth66=1362331083_1772f4693c9be3cb7361c13d205fe417&ext=.pdf
\indent	Another subject of research that is looking into using the Kinect is the simultaneous localization and mapping algorithm, used to create a 3D map of the world so that the robot can avoid collision with obstacles or walls. The SLAM problem could be solved using GPS if the robot is outside, but inside one needs to use wheel or visual odometry. Visual odometry determines the position and the orientation of the robot using the associated camera images, algorithms like Scale Invariant Feature Transformation (SIFT), used to find the interest points, and laser sensors, used to collect depth data. Since the Kinect has both the RGB camera and a laser sensor, this piece of technology is a good piece of hardware to use for robots computing the SLAM Algorithm. In the study conducted by the students in the Graduates School of Science and Technology, at Meiji University, they found that the Kinect worked well for this process for horizontal and straight movement, but they had errors when they tried to recreate an earlier experiment, this means that their algorithm successfully solves the initial problem, but accuracy fell over time.  \cite{cite2} They found that the issue was not with the Kinect, and that it could be solved using the Speed-Up Robust Feature algorithm (SURF) and Smirnov-Grubbs test to further improve the accuracy of their SLAM Algorithm. This study proved that the Kinect was a reasonable, inexpensive and non-special piece of equipment that is capable of preforming well in computer vision applications. \\
\indent	It seems as though the Kinect is a popular choice in current robotics and computer vision. This device is affordable, easily obtainable, and capable of a lot more than is expected from a video game add on. The Kinect combines a near-infrared laser pattern projector and an IR camera in one tool, and when combined with this eliminates the set up of some other configuration. The Kinect is also surprisingly accurate, requiring only some optimization software to make the results comparable to the results from a medium resolution SLR Stereo rig.

\section{}\bibliographystyle{plain}
\bibliography{FinalReport}

\section{Appendix} 
%Source Code: All source code (within reason) with some descriptive comments. The code does not count in the final page count

\end{document} 